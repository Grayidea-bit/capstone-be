# AI/code_analyzer.py
import httpx
import json
from typing import List, Dict, Any
from .setting import logger, redis_client, CACHE_TTL_SECONDS, generate_ai_content
from sklearn.metrics.pairwise import cosine_similarity
from .chat.embedding import embedding_function
from transformers import AutoTokenizer
import httpx
import base64
import numpy


class CodeAnalyzer:
    """
    ä¸€å€‹å…±ç”¨çš„ç¨‹å¼ç¢¼åˆ†æå™¨ï¼Œè² è²¬å»ºç«‹å’Œå¿«å–ç¨‹å¼ç¢¼åº«çš„çŸ¥è­˜åº«ã€‚
    å®ƒæœƒç²å–æ‰€æœ‰ Python æª”æ¡ˆï¼Œä¸¦æä¾›æŸ¥è©¢æª”æ¡ˆå…§å®¹çš„åŠŸèƒ½ã€‚
    """

    def __init__(
        self, owner: str, repo: str, access_token: str, client: httpx.AsyncClient
    ):
        self.owner = owner
        self.repo = repo
        self.access_token = access_token
        self.client = client
        # å¿«å–éµç¾åœ¨åŒ…å« commit SHAï¼Œä»¥å¯¦ç¾æ›´ç´°ç·»çš„å¿«å–
        self.latest_commit_sha = None

    async def _get_latest_commit_sha(self) -> str:
        """ç²å–æœ€æ–°çš„ commit SHA ä¸¦ç·©å­˜çµæœ"""
        if self.latest_commit_sha:
            return self.latest_commit_sha

        # å˜—è©¦å¾ Redis å¿«å–ç²å–æœ€æ–°çš„ commit SHA
        cache_key = f"latest_commit_sha:{self.owner}/{self.repo}"
        if redis_client:
            try:
                cached_sha = redis_client.get(cache_key)
                if cached_sha:
                    logger.info(f"å¾å¿«å–ä¸­ç²å–æœ€æ–°çš„ commit SHA: {cached_sha[:7]}")
                    self.latest_commit_sha = cached_sha
                    return cached_sha
            except Exception as e:
                logger.error(f"è®€å–æœ€æ–° commit SHA å¿«å–å¤±æ•—: {e}")

        response = await self.client.get(
            f"https://api.github.com/repos/{self.owner}/{self.repo}/commits",
            headers={"Authorization": f"Bearer {self.access_token}"},
            params={"per_page": 1},
        )
        response.raise_for_status()
        latest_sha = response.json()[0]["sha"]
        self.latest_commit_sha = latest_sha

        # å°‡æœ€æ–°çš„ commit SHA å­˜å…¥ Redis å¿«å–
        if redis_client:
            try:
                # è¨­å®šä¸€å€‹è¼ƒçŸ­çš„éæœŸæ™‚é–“ï¼Œä¾‹å¦‚ 5 åˆ†é˜ï¼Œä»¥ç¢ºä¿èƒ½åŠæ™‚ç²å–åˆ°æœ€æ–°çš„ commit
                redis_client.set(cache_key, latest_sha, ex=300)
            except Exception as e:
                logger.error(f"å¯«å…¥æœ€æ–° commit SHA å¿«å–å¤±æ•—: {e}")

        return latest_sha

    async def get_all_py_files(self) -> List[str]:
        """
        ç²å–ç¨‹å¼ç¢¼åº«ä¸­æ‰€æœ‰çš„ .py æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ï¼Œä¸¦é€²è¡Œå¿«å–ã€‚
        å¿«å–ç¾åœ¨èˆ‡æœ€æ–°çš„ commit SHA ç¶å®šã€‚
        """
        latest_commit_sha = await self._get_latest_commit_sha()
        cache_key_file_list = (
            f"code_analyzer:file_list:{self.owner}/{self.repo}:{latest_commit_sha}"
        )

        if redis_client:
            try:
                cached_files = redis_client.get(cache_key_file_list)
                print(f"*******************{cached_files}*************************")
                if cached_files:
                    logger.info(
                        f"å¾å¿«å–ä¸­ç²å–æª”æ¡ˆåˆ—è¡¨ (commit: {latest_commit_sha[:7]})"
                    )
                    return json.loads(cached_files)
            except Exception as e:
                logger.error(f"è®€å–æª”æ¡ˆåˆ—è¡¨å¿«å–å¤±æ•—: {e}")

        logger.info(
            f"æ­£åœ¨ç‚º {self.owner}/{self.repo} (commit: {latest_commit_sha[:7]}) ç²å–æª”æ¡ˆåˆ—è¡¨..."
        )
        tree_response = await self.client.get(
            f"https://api.github.com/repos/{self.owner}/{self.repo}/git/trees/{latest_commit_sha}?recursive=1",
            headers={"Authorization": f"Bearer {self.access_token}"},
        )
        tree_response.raise_for_status()
        tree_data = tree_response.json()

        py_files = [
            item["path"]
            for item in tree_data.get("tree", [])
            if item.get("type") == "blob" and item["path"].endswith(".py")
        ]

        if redis_client:
            try:
                # æª”æ¡ˆåˆ—è¡¨èˆ‡ commit SHA ç¶å®šï¼Œå¯ä»¥è¨­å®šè¼ƒé•·çš„éæœŸæ™‚é–“
                redis_client.set(
                    cache_key_file_list, json.dumps(py_files), ex=CACHE_TTL_SECONDS
                )
                logger.info(f"å·²å¿«å–æª”æ¡ˆåˆ—è¡¨ (commit: {latest_commit_sha[:7]})")
            except Exception as e:
                logger.error(f"å¯«å…¥æª”æ¡ˆåˆ—è¡¨å¿«å–å¤±æ•—: {e}")

        return py_files

    async def get_files_content(
        self, file_paths: List[str], ref: str = None
    ) -> Dict[str, str]:
        """
        ç²å–æŒ‡å®šæª”æ¡ˆè·¯å¾‘åˆ—è¡¨çš„å…§å®¹ï¼Œå„ªå…ˆå¾å¿«å–è®€å–ã€‚
        å¯ä»¥æŒ‡å®š ref (commit SHA, branch, tag) ä¾†ç²å–ç‰¹å®šç‰ˆæœ¬çš„æª”æ¡ˆå…§å®¹ã€‚
        """
        files_content_map = {}
        commit_sha_to_use = ref if ref else await self._get_latest_commit_sha()

        for file_path in file_paths:
            # å¿«å–éµåŒ…å« commit SHAï¼Œå¯¦ç¾ç‰ˆæœ¬åŒ–å¿«å–
            content_cache_key = f"code_analyzer:file_content:{self.owner}/{self.repo}:{commit_sha_to_use}:{file_path}"

            if redis_client:
                try:
                    cached_content = redis_client.get(content_cache_key)
                    if cached_content:
                        logger.info(
                            f"å¾å¿«å–ç²å–æª”æ¡ˆå…§å®¹: {file_path} @ {commit_sha_to_use[:7]}"
                        )
                        files_content_map[file_path] = cached_content
                        continue
                except Exception as e:
                    logger.error(f"è®€å–æª”æ¡ˆå…§å®¹å¿«å–å¤±æ•— for {file_path}: {e}")

            logger.info(
                f"æ­£åœ¨å¾ API ç²å–æª”æ¡ˆå…§å®¹: {file_path} @ {commit_sha_to_use[:7]}"
            )
            try:
                file_content_res = await self.client.get(
                    f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{file_path}?ref={commit_sha_to_use}",
                    headers={
                        "Authorization": f"Bearer {self.access_token}",
                        "Accept": "application/vnd.github.raw",
                    },
                )
                if file_content_res.status_code == 200:
                    content = file_content_res.text
                    files_content_map[file_path] = content
                    if redis_client:
                        try:
                            # ç‰¹å®šç‰ˆæœ¬çš„æª”æ¡ˆå…§å®¹æ˜¯æ°¸ä¹…ä¸è®Šçš„ï¼Œå¯ä»¥è¨­å®šè¼ƒé•·çš„éæœŸæ™‚é–“
                            redis_client.set(
                                content_cache_key, content, ex=CACHE_TTL_SECONDS
                            )
                        except Exception as e:
                            logger.error(f"å¯«å…¥æª”æ¡ˆå…§å®¹å¿«å–å¤±æ•— for {file_path}: {e}")
            except httpx.HTTPStatusError as e:
                logger.warning(
                    f"ç„¡æ³•ç²å–æª”æ¡ˆ {file_path} @ {commit_sha_to_use[:7]} çš„å…§å®¹: {e}"
                )

        return files_content_map

    async def file_embedding_similar(self, user_question: str, branch_name="main"):
        CHUNK_TOKEN = 512
        overlap_part = 10
        tokenizer = AutoTokenizer.from_pretrained("jinaai/jina-embeddings-v2-base-code")
        if not self.access_token:
            print("éŒ¯èª¤ï¼šæœªè¨­å®š GitHub access tokenã€‚")
            return

        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Accept": "application/vnd.github.v3+json",
        }

        cache_key_embedding_filelist = (
            f"code_analyzer:embedding_filelist:{self.owner}/{self.repo}/{branch_name}"
        )
        content_embedding = {}
        try:
            if redis_client:
                cached_content = redis_client.get(cache_key_embedding_filelist)

                if cached_content:
                    logger.info(f"å¾å¿«å–ç²å–embeddingæˆåŠŸæª”æ¡ˆ")
                    content_embedding = json.loads(cached_content)
                    content_embedding = {
                        name: numpy.array(vec)
                        for name, vec in content_embedding.items()
                    }

                else:
                    branch_info_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/branches/{branch_name}"
                    branch_info_res = await self.client.get(
                        branch_info_url, headers=headers
                    )
                    branch_info_res.raise_for_status()
                    latest_commit_sha = branch_info_res.json()["commit"]["sha"]

                    commit_info_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/git/commits/{latest_commit_sha}"
                    commit_info_res = await self.client.get(
                        commit_info_url, headers=headers
                    )
                    commit_info_res.raise_for_status()
                    tree_sha = commit_info_res.json()["tree"]["sha"]

                    tree_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/git/trees/{tree_sha}"
                    tree_res = await self.client.get(
                        tree_url, headers=headers, params={"recursive": "1"}
                    )
                    tree_res.raise_for_status()
                    tree_data = tree_res.json()

                    # éæ¿¾é™¤äº†è³‡æ–™å¤¾ä»¥å¤–çš„æ‰€æœ‰æª”æ¡ˆ
                    file_paths = [
                        item["path"]
                        for item in tree_data.get("tree", [])
                        if item.get("type") == "blob"
                        and not (
                            item["path"].endswith("png")
                            or item["path"].endswith("jpg")
                            or item["path"].endswith("jpeg")
                        )
                    ]

                    for path in file_paths:
                        print(path)
                        if path == "README.md" or path == ".env":
                            continue
                        content_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{path}"

                        content_res = await self.client.get(
                            content_url, headers=headers
                        )
                        content_res.raise_for_status()
                        content = content_res.json()
                        decoded_text = base64.b64decode(content["content"]).decode(
                            "utf-8"
                        )

                        # Chunk part
                        temp = decoded_text.split("\n")
                        sum_tokens = 0
                        embedding_list = []
                        embedding_text = ""
                        for index, i in enumerate(temp):
                            sum_tokens = sum_tokens + len(tokenizer.encode(i))
                            if sum_tokens < CHUNK_TOKEN:
                                embedding_text = embedding_text + "\n" + i
                            if sum_tokens >= CHUNK_TOKEN or index == len(temp) - 1:
                                embedding_list.append(
                                    embedding_function(embedding_text)
                                )
                                if index >= overlap_part:
                                    sum_tokens = 0
                                    embedding_text = ""
                                    for j in range(overlap_part):
                                        sum_tokens = sum_tokens + len(
                                            tokenizer.encode(temp[index - j] + "\n")
                                        )
                                        embedding_text = (
                                            embedding_text + "\n" + temp[index - j]
                                        )

                        content_embedding[path] = embedding_list
                    if redis_client:
                        content_embedding_json = {
                            name: numpy.array(tensor).tolist()
                            for name, tensor in content_embedding.items()
                        }
                        redis_client.set(
                            cache_key_embedding_filelist,
                            json.dumps(content_embedding_json),
                            ex=CACHE_TTL_SECONDS,
                        )

            # ReadMe info
            readme_response = await self.client.get(
                f"https://api.github.com/repos/{self.owner}/{self.repo}/readme",
                headers=headers,
            )
            readme_content = ""
            if readme_response.status_code == 200:
                readme_content = readme_response.text
            prompt = f"""
                            You are a technical language expander and rewriting assistant with contextual awareness.

                            You are given two inputs:
                            1. **{readme_content}**
                            2. **{user_question}**

                            Your task:
                            Rewrite the user's input into a detailed, professional, and context-rich **English** statement that clearly expresses what the user might be asking or referring to, based on the README.

                            ğŸ”¹ Always produce your entire output in **English only**, even if the user input is written in another language.

                            Follow these guidelines:
                            1. Use the README context to infer meaning.
                            2. Expand abbreviations and technical terms.
                            3. Clarify and infer the userâ€™s intended meaning.
                            4. Fill in missing details.
                            5. Translate any non-English input to fluent English.
                            6. Stay on-topic.
                            7. Preserve any code snippets exactly as written.
                            8. Do not explain your reasoning or translate back to the userâ€™s language.

                            Output format:
                            **Expanded:** (English rewritten version, 2â€“5 sentences)
                            **Code Snippet:** (Reproduce any code from the input; if none, leave blank)

                            Do not write anything except the output.
                            
                            """
            expanded_question = await generate_ai_content(prompt)
            reduce_text = []
            file_labels = []
            for k, v in content_embedding.items():
                for i in v:
                    reduce_text.append(i)
                    file_labels.append(k)

            reduce_text = numpy.array(reduce_text)
            question_embedding = numpy.array(embedding_function(expanded_question))

            max_similar = 0
            max_filename = ""
            for index, v in enumerate(reduce_text):
                similar = cosine_similarity(
                    question_embedding.reshape(1, -1), v.reshape(1, -1)
                )
                print(file_labels[index])
                print(similar)
                if similar > max_similar:
                    max_similar = similar
                    max_filename = file_labels[index]
            print()
            print(max_filename)
            print(max_similar)

            content_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{max_filename}"
            content_res = await self.client.get(content_url, headers=headers)
            content_res.raise_for_status()
            content = content_res.json()
            decoded_text = base64.b64decode(content["content"]).decode("utf-8")
            re_dict = {}
            re_dict[max_filename] = decoded_text
            return re_dict[max_filename]

        except httpx.HTTPStatusError as e:
            print(f"\néŒ¯èª¤ï¼šGitHub API è«‹æ±‚å¤±æ•—ã€‚ç‹€æ…‹ç¢¼: {e.response.status_code}")
            print(f"å›æ‡‰å…§å®¹: {e.response.text}")
        except Exception as e:
            print(f"\nç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
